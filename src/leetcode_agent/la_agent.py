import argparse
from enum import Enum
from typing import TypedDict, Annotated

import requests
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from langchain_openai import ChatOpenAI
from langgraph.checkpoint import MemorySaver
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages
from pydantic import BaseModel

from src.leetcode_agent.problem import LeetCodeProblem, read_problem
from src.leetcode_agent.prompt import (
    MAIN_CODE_GEN_SYSTEM_PROMPT,
    MAIN_CODE_GEN_USER_PROMPT,
    REGEN_BY_COMMENT_USER_PROMPT,
    REGEN_BY_ERROR_USER_PROMPT,
    TEST_CODE_GEN_SYSTEM_PROMPT,
    TEST_CODE_GEN_USER_PROMPT,
    TEST_CODE_VALIDATION_PROMPT,
    TEST_CODE_VALIDATION_WITH_ERRORS_PROMPT,
)
from src.leetcode_agent.util import (
    combine_test_with_code,
    extract_code,
    print_node_output, multiline_input
)

AI_TEST_REVISION_LIMIT = 2


class CodeExecutionResult(BaseModel):
    code: str
    stdout: str
    stderr: str
    has_error: bool


def execute_code(code: str) -> CodeExecutionResult:
    """Runs python code in my sandbox.  See README."""
    r = requests.post("http://127.0.0.1:8000/execute",
                      json={"code": code})
    if r.status_code == 200:
        r_json = r.json()
        return CodeExecutionResult(
            code=code,
            stdout=r_json.get("stdout"),
            stderr=r_json.get("stderr"),
            has_error=(r_json.get("returncode") != 0)
        )
    else:
        return CodeExecutionResult(
            code=code,
            stdout="",
            stderr=f"({r.json().get("detail")})",
            has_error=True
        )


class TestType(Enum):
    AI = 1
    EXAMPLES = 2

    def __str__(self):
        return self.name


class AgentState(TypedDict):
    # Chat history of solution-generating LLM
    main_coding_llm_messages: Annotated[list[BaseMessage], add_messages]

    # leetcode problem definition and example tests
    problem: LeetCodeProblem

    # Code generated by LLM
    main_code: str
    ai_test_code: str

    # Number of generations
    ai_test_code_revision: int
    main_code_revision: int
    main_code_revision_limit: int

    # Flags
    is_main_code_good: bool
    is_ai_test_code_good: bool
    skip_ai_test_code: bool

    # Test result
    last_test_result: CodeExecutionResult
    last_test_type: TestType

    # Human in the loop: feedback to give to main LLM
    has_human_interference: bool
    human_comment_on_main_code: str
    human_test_code: Annotated[str, lambda a, b: f"{a}\n{b}"]


def get_codegen_workflow() -> StateGraph:
    _llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.001)

    test_coding_llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.1)
    test_validation_llm = _llm

    _system_message = SystemMessage(content=MAIN_CODE_GEN_SYSTEM_PROMPT)
    main_coding_llm = (lambda messages: [_system_message] + messages) | _llm

    def node_generate_ai_test_code(state: AgentState):
        print("\n=== Generate ai test code ===\n")

        problem = state["problem"]
        revision = state.get("ai_test_code_revision") or 0

        messages = [
            SystemMessage(content=TEST_CODE_GEN_SYSTEM_PROMPT),
            HumanMessage(content=TEST_CODE_GEN_USER_PROMPT.format(
                problem_description=problem.problem_description,
                example_description=problem.example_description,
                solution_interface=problem.solution_interface
            ))
        ]
        content = test_coding_llm.invoke(messages).content
        revision += 1

        if not content:
            return {"ai_test_code": None, "ai_test_code_revision": revision}

        code = extract_code(content, "===test-start===", "===test-end===")
        return {
            "ai_test_code": code,
            "last_test_result": None,  # reset due to potential test regeneration
            "ai_test_code_revision": revision
        }

    def node_validate_test_code(state: AgentState):
        print("\n=== Validate ai test code ===\n")

        test_code = state["ai_test_code"]
        revision = state.get("main_code_revision") or 0

        if not test_code:
            return {
                "is_ai_test_code_good": False,
                "skip_ai_test_code": revision >= AI_TEST_REVISION_LIMIT
            }

        last_test_result = state["last_test_result"]
        if last_test_result and last_test_result.has_error and state["last_test_type"] == TestType.AI:
            # Main code is generated and run, but failed to pass the test code
            messages = [
                HumanMessage(content=TEST_CODE_VALIDATION_WITH_ERRORS_PROMPT.format(
                    problem_description=state["problem"].problem_description,
                    code=last_test_result.code,
                    ai_test_result_stderr=last_test_result.stderr
                ))
            ]
            content = _llm.invoke(messages).content
        else:
            # No main code yet; just asking for opinions about the test code
            messages = [
                HumanMessage(content=TEST_CODE_VALIDATION_PROMPT.format(
                    problem_description=state["problem"].problem_description,
                    test_code=state["ai_test_code"]
                ))
            ]
            content = test_validation_llm.invoke(messages).content

        if content.find("Validation result: yes") != -1:
            return {"is_ai_test_code_good": True}
        elif content.find("Validation result: no") != -1:
            return {
                "is_ai_test_code_good": False,
                "skip_ai_test_code": revision >= AI_TEST_REVISION_LIMIT
            }
        else:
            # TODO: logging or verbose; say this is unexpected but counted as passed
            print(content)
            return {"is_ai_test_code_good": True}

    def node_generate_main_code(state: AgentState):
        print("\n=== Generate main code ===\n")

        problem = state["problem"]
        revision = state.get("main_code_revision") or 0

        if (not state["main_code"]) or (not state["last_test_result"]):
            # No reflection since no main code is generated, or no (valid) tests were performed
            message = HumanMessage(
                content=MAIN_CODE_GEN_USER_PROMPT.format(
                    problem_description=problem.problem_description,
                    example_description=problem.example_description,
                    solution_interface=problem.solution_interface
                ))
            response = main_coding_llm.invoke([message])
        else:
            # Re-generate main code; reflect the previous error
            history_messages = state["main_coding_llm_messages"]
            last_test_result = state["last_test_result"]
            human_comment = state["human_comment_on_main_code"] or ""
            if last_test_result.has_error:
                message = HumanMessage(
                    content=REGEN_BY_ERROR_USER_PROMPT.format(
                        code=last_test_result.code,
                        error=last_test_result.stderr,
                        human_comment=human_comment
                    ))
            else:
                message = HumanMessage(
                    content=REGEN_BY_COMMENT_USER_PROMPT.format(
                        human_comment=human_comment
                    ))
            response = main_coding_llm.invoke(history_messages + [message])

        content = response.content
        revision += 1

        if not content:
            code = None
        else:
            code = extract_code(content, "===code-start===", "===code-end===")
        return {
            "main_coding_llm_messages": [message, response],
            "main_code": code,
            "main_code_revision": revision
        }

    def node_test_main_with_examples(state: AgentState):
        print("\n=== Test main code w/ examples ===\n")

        main_code = state["main_code"]
        if not main_code:
            return {"is_main_code_good": False}

        example_test_code = state["problem"].example_test_code

        if state["human_test_code"]:
            example_test_code += "\n" + state["human_test_code"]

        code_result = execute_code(
            combine_test_with_code(main_code, example_test_code)
        )
        return {
            "is_main_code_good": not code_result.has_error,
            "last_test_result": code_result,
            "last_test_type": TestType.EXAMPLES
        }

    def node_test_main_with_ai_tests(state: AgentState):
        print("\n=== Test main code w/ AI tests ===\n")

        if state.get("skip_ai_test_code", False):
            return {}

        main_code = state["main_code"]
        if not main_code:
            return {"is_main_code_good": False}

        code_result = execute_code(
            combine_test_with_code(main_code, state["ai_test_code"])
        )
        return {
            "is_main_code_good": not code_result.has_error,
            "last_test_result": code_result,
            "last_test_type": TestType.AI
        }

    def node_give_answer(state: AgentState):
        print("\n=== Here is the result ===\n")

        print(">>>> SOLUTION <<<<")
        print(state["main_code"])
        print(">> SOLUTION END <<\n\n")

        if state["is_main_code_good"]:
            print("This code should work.  Please verify.")
        else:
            print(f"This code didn't pass the test from {state["last_test_type"]}:\n")
            print(state["last_test_result"].stderr, end="\n\n")

        if state.get("skip_ai_test_code", False):
            print("BTW, I tried AI-generated tests but skipped, because the tests themselves have problems.\n")

    def node_finalize(state: AgentState):
        # resource cleanup or something like that
        pass

    def to_regenerate_ai_test_code(state: AgentState) -> str:
        revision = state.get("ai_test_code_revision") or 0
        if revision >= AI_TEST_REVISION_LIMIT:
            return "no"

        if state.get("is_ai_test_code_good", False):
            return "no"
        else:
            return "yes"

    def to_regenerate_main_code(state: AgentState) -> str:
        revision = state.get("main_code_revision") or 0
        revision_limit = state.get("main_code_revision_limit", 4)
        if revision >= revision_limit:
            return "no"

        if state.get("is_main_code_good", False):
            return "no"
        else:
            return "yes"

    def has_human_interference(state: AgentState) -> str:
        if state.get("has_human_interference"):
            return "yes"
        else:
            return "no"

    workflow = StateGraph(AgentState)

    workflow.add_node("generate_ai_test_code", node_generate_ai_test_code)
    workflow.add_node("validate_test_code", node_validate_test_code)
    workflow.add_node("generate_main_code", node_generate_main_code)
    workflow.add_node("test_with_examples", node_test_main_with_examples)
    workflow.add_node("test_with_ai", node_test_main_with_ai_tests)
    workflow.add_node("give_answer", node_give_answer)
    workflow.add_node("finalize", node_finalize)

    workflow.set_entry_point("generate_ai_test_code")

    workflow.add_edge("generate_ai_test_code", "validate_test_code")
    workflow.add_conditional_edges(
        source="validate_test_code",
        path=to_regenerate_ai_test_code,
        path_map={
            "yes": "generate_ai_test_code",
            "no": "generate_main_code"
        }
    )
    workflow.add_edge("generate_main_code", "test_with_examples")
    workflow.add_conditional_edges(
        source="test_with_examples",
        path=to_regenerate_main_code,
        path_map={
            "yes": "generate_main_code",
            "no": "test_with_ai"
        }
    )
    workflow.add_conditional_edges(
        source="test_with_ai",
        path=to_regenerate_main_code,
        path_map={
            "yes": "validate_test_code",  # can be a mistake in AI test code
            "no": "give_answer"
        }
    )
    workflow.add_conditional_edges(
        source="give_answer",
        path=has_human_interference,
        path_map={
            "yes": "generate_main_code",
            "no": "finalize"
        }
    )
    workflow.add_edge("finalize", END)

    return workflow


def solve_leetcode(leetcode_problem: LeetCodeProblem):
    config = {"configurable": {"thread_id": "user-24601-conv-1337"}}

    graph = get_codegen_workflow().compile(
        checkpointer=MemorySaver(),
        interrupt_after=["give_answer"]
    )

    # Run the graph with the initial input
    initial_state = {"problem": leetcode_problem, "main_code_revision_limit": 4}
    for s in graph.stream(initial_state, config=config):
        print_node_output(s)  # TODO: verbose

    # XXX: This pattern only works when the upcoming state at the interruption
    #      is not END, so we can distinguish END from interruption.
    #      Another pattern is to use `while True:` with conditional break by user_input
    while graph.get_state(config).next:
        user_test = multiline_input("Add more asserting tests if necessary")
        user_comment = input('Provide feedback to revise the code (just enter if none):')

        if user_test or user_comment:
            state = graph.get_state(config).values
            current_main_code_revision_limit = state["main_code_revision_limit"]

            graph.update_state(
                config=config,
                values={
                    "has_human_interference": True,
                    "human_comment_on_main_code": user_comment,
                    "human_test_code": user_test,
                    "main_code_revision_limit": current_main_code_revision_limit + 2
                }
            )
        else:
            graph.update_state(
                config=config,
                values={
                    "human_comment_on_main_code": "",
                    "has_human_interference": False
                }
            )

        for s in graph.stream(None, config=config):
            print_node_output(s)  # TODO: verbose


def main():
    parser = argparse.ArgumentParser(description="Solve a leetcode problem")
    parser.add_argument("dir_name", type=str,
                        help="The directory containing necessary files of a leetcode problem.")
    args = parser.parse_args()

    # solve_leetcode(PROBLEM_1)
    leetcode_problem = read_problem(args.dir_name)
    solve_leetcode(leetcode_problem)


if __name__ == '__main__':
    main()
